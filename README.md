# ğŸ¦… DataHawk - AI-Powered Web Scraper

DataHawk is an intelligent web scraping tool that leverages AI to extract, parse, and organize data from any websiteâ€”no matter how complex. Whether you need e-commerce prices, news articles, social media trends, or custom datasets, DataHawk delivers with precision and speed.

## âœ¨ Key Features

- âœ… **AI-Powered Extraction** â€“ Handles dynamic content, JavaScript-heavy sites, and anti-scraping measures
- âœ… **No-Code Friendly** â€“ Simple Streamlit interface with smart automation
- âœ… **Customizable & Scalable** â€“ From one-time scrapes to large-scale data pipelines
- âœ… **Ethical & Stealthy** â€“ Respects robots.txt and mimics human browsing patterns
- âœ… **Google Gemini AI** â€“ Fast, accurate content parsing with natural language descriptions

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8 or higher
- Google Gemini API key (free tier available)

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd DataHawk
   ```

2. **Run the setup script:**
   ```bash
   python setup.py
   ```

3. **Configure your API key:**
   - Get your free Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Edit `.env` file and replace `your_gemini_api_key_here` with your actual API key

4. **Start DataHawk:**
   ```bash
   streamlit run main.py
   ```

### Manual Installation

If you prefer manual setup:

```bash
# Install dependencies
pip install -r requirements.txt

# Copy environment template
copy .env.example .env

# Edit .env file with your API keys
# Then run:
streamlit run main.py
```

## ğŸ“– How to Use

1. **Enter Website URL** - Input any website you want to scrape
2. **Fetch Data** - Click to scrape and extract the website content
3. **Describe What to Parse** - Tell the AI what specific data you want to extract
4. **Parse Content** - Let AI analyze and extract the requested information
5. **Download Results** - Save your extracted data as a text file

### Example Parsing Descriptions

- "Extract all product names and prices"
- "Get all email addresses and phone numbers"
- "Find all article titles and publication dates"
- "Extract company information and contact details"
- "Get all job listings with salaries and locations"

## ğŸ› ï¸ Technical Architecture

### Core Components

- **`main.py`** - Streamlit web interface with modern UI
- **`scrape.py`** - Web scraping using Selenium + BrightData proxy
- **`parse.py`** - AI-powered content parsing using Google Gemini
- **`setup.py`** - Automated installation and configuration

### Technologies Used

- **Frontend:** Streamlit (modern web UI)
- **Web Scraping:** Selenium WebDriver + BeautifulSoup4
- **AI Processing:** Google Gemini 1.5 Flash via API
- **Proxy Service:** BrightData SuperProxy (optional, for premium scraping)

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# Required: Google Gemini API Configuration
GEMINI_API_KEY=your_actual_api_key

# Optional: BrightData Proxy (for advanced scraping)
BRIGHTDATA_USERNAME=your_username
BRIGHTDATA_PASSWORD=your_password
BRIGHTDATA_ENDPOINT=brd.superproxy.io:9515
```

### API Limits (Gemini Free Tier)

- **Requests:** 15 per minute
- **Tokens:** 1 million per day
- **Models:** Gemini 1.5 Flash (fast and efficient)

## ğŸ”’ Security & Privacy

- All API keys are stored in environment variables
- No credentials hardcoded in source code
- Proxy credentials configurable via environment
- Respects website robots.txt and rate limits

## ğŸ“ Project Structure

```
DataHawk/
â”œâ”€â”€ main.py              # Streamlit web interface
â”œâ”€â”€ scrape.py            # Web scraping logic
â”œâ”€â”€ parse.py             # AI parsing with Gemini
â”œâ”€â”€ setup.py             # Automated setup script
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env.example         # Environment template
â”œâ”€â”€ .env                 # Your configuration (private)
â”œâ”€â”€ README.md            # This file
â””â”€â”€ .gitignore          # Git ignore rules
```

## ğŸ¤ Contributing

DataHawk is designed for developers, researchers, marketers, and data analysts who need fast, reliable, and adaptable web scraping.

## ğŸ“„ License

This project is open source. Please use responsibly and respect website terms of service.

## ğŸ†˜ Troubleshooting

### Common Issues

1. **Import errors** - Run `python setup.py` to install dependencies
2. **API key errors** - Make sure your Gemini API key is correct in `.env`
3. **Scraping failures** - Some websites may block automated access
4. **Parsing issues** - Try more specific descriptions for better results

### Getting Help

- Check the sidebar help in the application
- Review parsing examples for better prompts
- Ensure your API key has sufficient quota

---

ğŸš€ **Get the data you wantâ€”automagically with DataHawk!** ğŸ¦…
